{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les méthodes de modification de logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is a member of the German National Socialist Party (Bundeswehr), which is a party of the Social Democratic Party (Bundeswehr), which is a party of the Christian Democratic Party (Bundeswehr), which is a'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On charge le modèle \n",
    "model_name = \"gpt2\" # Vous pouvez jouer avec différents modèles selon la puissance de votre machine\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Aussois is\" # Texte à tester ici\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False) # On encode le texte en tokens\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is one of the most well-known and well-researched and well-researched scientists in the world. He is also one of the most well-known and well-researched and well-researched'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**inputs, num_beams=5, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling ancestral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aussois is a British actor – he doesn't like to play a bad boy, but he knows what's in his head.\\n\\nIt's not a bad game: you just gotta make him get into the act with every word.\\n\\nWe will all\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top_k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is at this moment on trial for an offence connected with an alleged hate crime, allegedly carried out in the name of white supremacy.\\n\\nAn hour after the verdict was read, he was arrested and sent to a \"non-secure address\". The investigation'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, top_k=40, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleus (top_p) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aussois is one of the few countries to offer a free WiFi access to customers.\\n\\nIt can now also access Wi-Fi to customers with a cellular data plan (sold separately).\\n\\nThe service is free, though it's priced at an additional $\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, top_p=0.9, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally typical sampling (typical_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is a name given to a group of individuals, sometimes referred to as \"the humanoids\", that were sent out of time to escape their original forms.\\n\\nSome of the earliest known Homo sapiens were Homo erectus (sometimes abbreviated Homo erect'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, typical_p = 0.9, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$-sampling (eta_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aussois is a man of many senses who doesn't mind seeing others, but when something's being done, he'll go for it, making it possible for others to be seen. If he wants to see a man, he'll do it, but they'll\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, eta_cutoff=0.003, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aussois is an English football prodigy in his fifth year of playing. He was a member on the German national team during their World Cup run back at Leipzig, Germany and has played alongside him twice for BND's European Championship championship competition this past summer\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, eta_cutoff=0.003, max_new_tokens=50, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n",
    "On peut cumuler tous ces paramètres afin d’influencer la génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is not the only one who has made a name for himself in the game. In fact, he is one of the most successful players in the game.\\n\\nAussois is the only one who has made a name for himself in the game'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ancestral Sampling avec plusieurs beams\n",
    "output = model.generate(**inputs, num_beams=5, do_sample=True, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is not the only one who has taken up the cause. In fact, it is the only one who has taken up the cause. In fact, it is the only one who has taken up the cause. In fact, it is the only one who'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top-k avec plusieurs beams\n",
    "output = model.generate(**inputs, num_beams=5, do_sample=True, top_k=40, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is a game of basketball. It is about creating an edge for your opponents when it comes to the ball.\\n\\nFor the last few seasons, there have been three of the five players that were asked to put up 20 points in the NBA Finals on'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un peu de tout\n",
    "output = model.generate(**inputs, num_beams=1, do_sample=True, top_k=40, top_p=0.9, eta_cutoff=0.003, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aussois is in the US for a period of time and has been on staff there since 2008, at which point he became an associate Professor Emeritus.\\n\\n\\n...as well as being chairman of the International Institute (IIC) from 2010 to 2012; I'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testez tout\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    num_beams=1, \n",
    "    do_sample=True, \n",
    "    top_k=40, \n",
    "    top_p=0.9, \n",
    "    temperature=0.9,\n",
    "    eta_cutoff=0.003, \n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=50, \n",
    "    pad_token_id=tokenizer.eos_token_id) \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesures de diversité de la génération\n",
    "Ici, on va mesurer à quel point générer plusieurs fois va donner des textes différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sacrebleu\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_bleu(generated_texts):\n",
    "    # Calcul du score BLEU pour chaque paire de textes\n",
    "    pairwise_bleu_scores = []\n",
    "    for i in range(len(generated_texts)):\n",
    "        for j in range(i+1, len(generated_texts)):\n",
    "            hypothesis = generated_texts[i]\n",
    "            references = [generated_texts[j]]\n",
    "            bleu = sacrebleu.corpus_bleu([hypothesis], [references])\n",
    "            pairwise_bleu_scores.append(bleu.score)\n",
    "            #print(f\"score BLEU entre la génération {i} et {j}: {bleu.score:.2f}\")\n",
    "\n",
    "    # Calcul du BLEU moyen en tant que mesure de diversité\n",
    "    average_bleu = sum(pairwise_bleu_scores) / len(pairwise_bleu_scores)\n",
    "    return average_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "Aussois is one of the most well-known and well-researched and well-researched and well-researched and well-researched and well-researched and well-researched and\n",
      "\n",
      "Sample 1:\n",
      "Aussois is a member of the European Parliament and a member of the Council of Europe. He is also a member of the European Parliament and a member of the Council of Europe. He is also a member of the European Parliament and a member of the Council of Europe\n",
      "\n",
      "Sample 2:\n",
      "Aussois is one of the most well-known and well-known names in the world. He is also one of the most well-known and well-known names in the world. He is also one of the most well-known and well-known names\n",
      "\n",
      "Sample 3:\n",
      "Aussois is one of the most well-known and well-known names in the history of the world. He is one of the most well-known and well-known names in the history of the world. He is one of the most well-known and\n",
      "\n",
      "Sample 4:\n",
      "Aussois is one of the most well-known and well-researched and well-researched people in the world. He is one of the most well-known and well-researched people in the world. He is one\n",
      "\n",
      "Score BLEU moyen :25.946889617659224\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Aussois is\"  # Prompt\n",
    "\n",
    "# Encodage du prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# On génère plusieurs textes\n",
    "num_samples = 5  # Nombre de textes\n",
    "max_new_tokens = 50  # Nombre de tokens à générer\n",
    "generated_texts = []\n",
    "\n",
    "# Boucle de génération\n",
    "for i in range(num_samples):\n",
    "    \"\"\" Optionnel, on peut ajouter une seed pour reproduire toujours les mêmes \n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "    \"\"\"\n",
    "\n",
    "    # Génération de textes avec différents paramètres\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=5,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        #top_k=40,       \n",
    "        temperature=0.7,\n",
    "        #top_p=0,\n",
    "        #eta_cutoff=0,\n",
    "        #typical_p=0.9,\n",
    "        #repetition_penalty=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    # Décodage des tokens\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_texts.append(generated_text)\n",
    "    print(f\"Sample {i}:\\n{generated_text}\\n\")\n",
    "\n",
    "avg = calculate_average_bleu(generated_texts)\n",
    "print(f\"Score BLEU moyen :{avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU va mesurer à quel point les phrases sont similaires, plus il est bas, plus les générations sont diverses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam-search curse\n",
    "À partir d’une certaine taille, augmenter le nombre de beams va dégrader les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Bleu moyen pour 1 beams : 3.31441\n"
     ]
    }
   ],
   "source": [
    "# On génère plusieurs textes\n",
    "num_samples = 5  # Nombre de textes\n",
    "max_new_tokens = 50  # Nombre de tokens à générer\n",
    "generated_texts = []\n",
    "\n",
    "nb_beams = [1]#, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "for j in nb_beams:\n",
    "    # Boucle de génération\n",
    "    for i in range(num_samples):\n",
    "        \"\"\" Optionnel, on peut ajouter une seed pour reproduire toujours les mêmes \n",
    "        torch.manual_seed(i)\n",
    "        np.random.seed(i)\n",
    "        random.seed(i)\n",
    "        \"\"\"\n",
    "\n",
    "        # Génération de textes avec différents paramètres\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=j,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            #top_k=40,       \n",
    "            temperature=0.7,\n",
    "            #top_p=0,\n",
    "            #eta_cutoff=0,\n",
    "            #typical_p=0.9,\n",
    "            #repetition_penalty=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        # Décodage des tokens\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "        #print(f\"Sample {i}:\\n{generated_text}\\n\")\n",
    "    avg = calculate_average_bleu(generated_texts)\n",
    "    print(f\"Score Bleu moyen pour {j} beams : {avg:.5f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le score Bleu moyen augmente au fur et a mesure que le nombre de beams augmente, donnant donc des générations de plus en plus similaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions d’arrêt du beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Bayes Risk Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthieudubois/miniconda3/envs/Text_Generation/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"model = \n",
    "tokenizer = \"\"\"\n",
    "\n",
    "prompt = \"Aussois is\"\n",
    "# Encodage du prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "#Génération des références \n",
    "nb_best = 5\n",
    "best_set = []\n",
    "for i in range(nb_best):\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        # Ajouter arguments de génération ici\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    best_set.append(generated_text)\n",
    "\n",
    "# Génération diverses \n",
    "nb_divers = 30\n",
    "monte_carlo_set = []\n",
    "for i in range(nb_divers):\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        # Ajouter arguments de génération ici\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    monte_carlo_set.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def select_best_candidate(best_set, monte_carlo_set): # Ici on cherche le candidat qui synthétise le mieux les autres selon la métrique BLEU\n",
    "    scores = []\n",
    "    for candidate in best_set:\n",
    "        pairwise_bleu_scores = []\n",
    "        for reference_text in monte_carlo_set:\n",
    "            bleu = sacrebleu.corpus_bleu([candidate], [[reference_text]])\n",
    "            pairwise_bleu_scores.append(bleu.score)\n",
    "\n",
    "        # Calcul du BLEU moyen pour ce candidat\n",
    "        average_bleu = sum(pairwise_bleu_scores) / len(pairwise_bleu_scores)\n",
    "        scores.append(average_bleu)\n",
    "\n",
    "    # On sélectionne le candidat avec le score moyen le plus élevé, celui qui correspond le plus à tous les autres\n",
    "    best_index = max(range(len(scores)), key=lambda idx: scores[idx])\n",
    "\n",
    "    return best_set[best_index], scores[best_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La meilleure phrase est :\n",
      "Aussois is a member of the German National Socialist Party (Bundeswehr), \n",
      "avec un BLEU moyen de 11.13099\n"
     ]
    }
   ],
   "source": [
    "candidat, score = select_best_candidate(best_set, monte_carlo_set)\n",
    "print(f\"La meilleure phrase est :\\n{candidat} \\navec un BLEU moyen de {score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score: 0.0005\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# On charge le modèle chargé de scorer nos textes\n",
    "# C’est un modèle très simple chargé de donner un score positif et négatif aux textes, chacun entre 0 et 1\n",
    "scoring_model_name = \"siebert/sentiment-roberta-large-english\"\n",
    "scoring_tokenizer = AutoTokenizer.from_pretrained(scoring_model_name)\n",
    "scoring_model = AutoModelForSequenceClassification.from_pretrained(scoring_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scoring_model.to(device)\n",
    "scoring_model.eval()\n",
    "\n",
    "def get_discriminator_score(texts):\n",
    "    \"\"\"\n",
    "    On calcule le sentiment (positif ou négatif)\n",
    "    \"\"\"\n",
    "    inputs = scoring_tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = scoring_model(**inputs)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    positive_scores = probabilities[:, 1]  # Index 1 représente le score \"positif\"\n",
    "    negative_scores = probabilities[:, 0]  # Index 0 représente le score \"négatif\"\n",
    "\n",
    "    # Il faut choisir si l’on veut un texte positif ou négatif en commentant la ligne inutile\n",
    "    scores = positive_scores               # On veut que le texte généré soit plutôt positif\n",
    "    #scores = negative_scores               # On veut que le texte généré soit plutôt négatif\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Exemple\n",
    "text = \"This movie was terrible\"\n",
    "score = get_discriminator_score([text])\n",
    "print(f\"Sentiment score: {score[0].item():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|██████████| 50/50 [04:10<00:00,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "This movie was a huge hit. I think it's one of the best movies ever made, and that is why we are so proud to be able do this film again.\"\n",
      "\"I'm really excited about what you're doing with The Hobbit: An Unexpected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mcts_script import main\n",
    "\n",
    "# Arguments\n",
    "c = 1.0              # Constante d’exploration, plus haut = plus d’exploration des noeuds les moins visités\n",
    "alpha = 1.0          # Priorité donnée aux probas du modèle ou au score que l’on renvoie\n",
    "temperature = 0.7    # Température utilisée pendant la génération\n",
    "penalty = 1.1        # Pénalité de répétition\n",
    "num_it = 50          # Nombre d’itérations MCTS par token\n",
    "prompt_text = \"This movie was\"\n",
    "\n",
    "\n",
    "# Call the main function with parameters\n",
    "main(c, alpha, temperature, penalty, num_it, get_discriminator_score, prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculs de perplexité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Méthode 1, avec tout le contexte à chaque fois \n",
    "def compute_token_by_token_ppl(model, encodings):\n",
    "    input_ids = encodings.input_ids\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len > model.config.n_positions:\n",
    "        return \"Ce texte est trop long pour la fenêtre de contexte du modèle\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    \n",
    "    logits = outputs.logits  # shape: [batch_size, seq_length, vocab_size]\n",
    "    \n",
    "    # Shift pour prédire le token i à la position i-1\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    # On gather pour regarder les probabilités du token qui est renvoyé\n",
    "    token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    average_nll = -token_log_probs.mean() \n",
    "    token_by_token_ppl = torch.exp(average_nll)\n",
    "    return token_by_token_ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_context_window_ppl(model, encodings, window_size):\n",
    "    input_ids = encodings.input_ids\n",
    "    seq_len = input_ids.size(1)\n",
    "    max_length = window_size\n",
    "\n",
    "    window_losses = []\n",
    "    for start_idx in range(0, seq_len, max_length):\n",
    "        end_idx = min(start_idx + max_length, seq_len)\n",
    "        window_input_ids = input_ids[:, start_idx:end_idx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_input_ids, labels=window_input_ids)\n",
    "        \n",
    "        # outputs.loss représente la NLL moyenne sur tous les tokens dans la fenêtre\n",
    "        window_loss = outputs.loss\n",
    "        window_losses.append(window_loss)\n",
    "\n",
    "    average_loss = torch.stack(window_losses).mean()\n",
    "    context_window_ppl = torch.exp(average_loss)\n",
    "    return context_window_ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_with_half_window_context(model, encodings, window_size):\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    input_ids = encodings.input_ids\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    start_positions = range(0, seq_len, half_window)\n",
    "    for start_pos in tqdm(start_positions):\n",
    "        end_pos = start_pos + window_size\n",
    "        # Si on a plus assez de tokens, on break ou on peut calculer la ppl des tokens restants en mettant end_pos = seq_len\n",
    "        if end_pos > seq_len:\n",
    "            end_pos = seq_len\n",
    "            break\n",
    "\n",
    "        # On choisit notre fenêtre\n",
    "        window_input_ids = input_ids[:, start_pos:end_pos]\n",
    "\n",
    "        # Seule la deuxième moitié de la fenêtre de contexte sert à scorer notre texte\n",
    "        target_ids = window_input_ids.clone()\n",
    "        # On masque la première moitié du texte (-100 est une valeur arbitraire)\n",
    "        target_ids[:, :half_window] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        if end_pos == seq_len:\n",
    "            break\n",
    "\n",
    "    # On récupère la perplexité moyenne\n",
    "    average_nll = torch.stack(nlls).mean()\n",
    "    ppl = torch.exp(average_nll)\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1147 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "text = \"On a crisp autumn morning, Eleanor stepped outside her small cottage at the edge of the forest and began walking down a narrow path lined with amber and russet leaves. The world had changed colors overnight, as if an unseen painter had swept across the landscape with a palette of warm hues. She breathed in the scent of damp earth, distant pine, and the faint aroma of woodsmoke drifting from a neighbor’s chimney. Although the air carried a slight chill, the day promised gentle sunshine and light winds, perfect for a long and thoughtful stroll. As she continued along the winding trail, Eleanor recalled the stories her grandfather had told her when she was a child—tales of hidden groves and forgotten clearings deep in the woods, where ancient trees whispered secrets to one another. He had insisted that if one listened carefully enough, the wind through the branches carried voices from centuries past. She had never been entirely sure whether to believe him, but as she walked, she allowed herself the luxury of imagining these old legends might hold some truth. There was comfort in thinking that one’s ancestors might leave behind faint echoes, lingering in the quiet corners of nature. About half a mile in, Eleanor reached a small stream. Its waters, clear and cold, danced over smooth stones, creating a soft, melodic murmur. She paused to watch leaves float downstream, each one a tiny vessel drifting toward unknown destinations. The sunlight, filtering through the half-bare branches, reflected in bright spots off the ripples, reminding her that beauty often lay in small, transient details. She took her time before moving on, feeling as if each moment was a gift she shouldn’t rush. Not long after crossing the stream via a makeshift wooden plank, she came across a clearing she did not recognize. It was shaped like an oval and ringed with young birch trees whose bark gleamed pale against the darker backdrop of firs and oaks. In the center stood a solitary stone bench, weathered and covered in moss, as though it had been placed there by someone who valued solitude. Eleanor brushed off some of the moss and sat, resting her legs and taking in the silent theater around her. No birds chirped, no squirrels chattered. It was as if this spot had declared itself a sanctuary from noise. While resting, she thought about the countless times she had ventured outdoors in search of nothing in particular: just the quiet companionship of trees, the patient passage of clouds, and the sound of her own footsteps on the trail. She considered how, in these quiet moments, she often found a clarity that eluded her in the hustle of daily life. Back at her cottage, there were chores to be done, letters to answer, and errands waiting. Out here, these demands receded, leaving room for the warmth of memories and the subtle interplay of time and stillness. Refreshed by the pause, Eleanor stood and continued forward, leaving the clearing behind. Eventually, the path widened, and she found herself walking alongside an old stone wall, partially collapsed in places. Vines and moss had claimed it, weaving new textures and patterns that hinted at the slow, persistent artistry of nature over generations. She liked to imagine who might have built the wall—farmers long ago clearing land, or perhaps villagers marking a boundary. The wall, now broken and quiet, bore silent witness to a past she could only guess at. As noon approached, the sunlight grew warmer, and the forest seemed to awaken. A distant woodpecker tapped at a trunk, small birds fluttered between branches, and a gentle breeze carried the distant laughter of someone working in a nearby orchard. Eleanor knew there was a village not far beyond the forest’s border, where life continued its pleasant rhythm: apples harvested, bread baked, stories swapped over steaming cups of tea. Soon, she would turn back toward her cottage, but not just yet. The day still had hours left to unfold. After another half-hour of walking, she reached a hillside that offered a view of rolling fields beyond the trees. Patches of farmland, dotted with hay bales, stretched toward a line of distant hills. A single hawk circled overhead, its keen eyes scanning the ground below for a quick meal. Eleanor watched the hawk’s flight, feeling a quiet admiration for its independence and grace. When she finally turned around to retrace her steps home, she found the forest just as welcoming as before. The path felt familiar but not stale; rather, it was like greeting an old friend. She noticed details she had missed earlier—a cluster of mushrooms at the base of a beech tree, the gentle slope of the trail as it curved around a thicket. Each step brought her closer to the cottage, and with it the ordinary tasks of her life, but she carried within her a renewed sense of peace. By the time Eleanor stepped through her front door, the golden afternoon light had begun to angle across the floorboards. She placed a kettle on the stove, choosing a fragrant herbal blend for her tea. Waiting for the water to boil, she looked out the window at the edge of the forest, grateful that she had taken the time to wander among the trees. There was a quiet magic in that forest—subtle, unassuming, yet undeniably present. It asked for nothing and offered everything: a calm place to think, to listen, to remember. And tomorrow, or perhaps the day after, she might return to discover something new, or to simply be, wrapped in the gentle hush of autumn leaves and whispered histories.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ce texte est trop long pour la fenêtre de contexte du modèle'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_token_by_token_ppl(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00,  4.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.677274703979492"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity_with_half_window_context(model, inputs, window_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.246902465820312"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_context_window_ppl(model, inputs, window_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient différents résultats de perplexité à cause des différentes façons de calculer. Sur les texte très longs, ces différences peuvent devenir très grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text_Generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
