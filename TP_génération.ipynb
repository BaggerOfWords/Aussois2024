{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Génération de Texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus d’information et si vous souhaitez jeter un coup d’oeil au code de huggingface, il est disponible ici : \n",
    "https://github.com/huggingface/transformers/tree/main/src/transformers/generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge le modèle \n",
    "model_name = \"gpt2\" # Vous pouvez jouer avec différents modèles selon la puissance de votre machine\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Aussois is\" # Texte à tester ici\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device) # On encode le texte en tokens\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour changer le modèle utilisé (ici \"gpt2\"), vous pouvez vous balader sur huggingface, par exemple voir les modèles les plus téléchargés ici :\n",
    "https://huggingface.co/models?pipeline_tag=text-generation&sort=trending.\n",
    "Attention cependant à la mémoire. GPT2 tourne très bien sur (presque) n’importe quelle machine, alors que des modèles plus gros seront plus gourmands. Si vous utilisez une machine avec un gpu, pensez à instancier device = \"cuda\" et y déplacer le modèle ainsi que le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, num_beams=5, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans toute la suite du TP, on définit pad_token_id=eos.token_id, cela signifie que l’on fait de la génération ouverte. En faisant cela, on indique au modèle que générer du vide est équivalent à terminer la génération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut augmenter le nombre de beams, augmenter la longueur, et renvoyer plus qu’une séquence avec num_return_sequences\n",
    "\n",
    "output = model.generate(**inputs, num_beams=7, max_new_tokens=50, num_return_sequences=3, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "\n",
    "for generated in output :\n",
    "    print(tokenizer.decode(generated)) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le beam search est déterministe, faire tourner les cellules précédentes va donc toujours renvoyer la même chose. On voit bien que les séquences renvoyées sont très proches, on est dans un cas où certains préfixes sont bien meilleurs que les autres aux yeux du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling ancestral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le sampling est lui stochastique, et va donc donner des résultats différents à chaque instance. Les résultats peuvent être très surprenants. N’hésitez pas à relancer la cellule suivante jusqu’à avoir des phrases très surprenantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loop in range(5):\n",
    "    output = model.generate(**inputs, num_beams=1, do_sample=True, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "    text = tokenizer.decode(output[0]) # On convertit les tokens en mots\n",
    "    print(f\"Génération {loop} : \\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top_k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, top_k=40, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleus (top_p) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, top_p=0.9, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally typical sampling (typical_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, typical_p = 0.9, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$-sampling (eta_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, eta_cutoff=0.003, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, num_beams=1, do_sample=True, eta_cutoff=0.003, max_new_tokens=50, repetition_penalty=1.2, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n",
    "On peut cumuler tous ces paramètres afin d’influencer la génération"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment est-ce que Huggingface arrive à cumuler le Beam Search déterministe avec du sampling stochastique ? Vous pouvez regarder plus en détail ce qui se passe dans le code ici :\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/generation/beam_search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ancestral Sampling avec plusieurs beams\n",
    "output = model.generate(**inputs, num_beams=5, do_sample=True, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut regarder les résultats pour les différents beams\n",
    "\n",
    "output = model.generate(**inputs, num_beams=10, do_sample=True, max_new_tokens=50, num_return_sequences=3, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "for generation in output : \n",
    "    print(tokenizer.decode(generation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k avec plusieurs beams\n",
    "output = model.generate(**inputs, num_beams=5, do_sample=True, top_k=40, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand on cumule les troncations, on applique d’abord top-k puis top-p d’après https://huggingface.co/blog/how-to-generate, mais qu’est-ce qui se passe quand on les cumules toutes ? Vous pouvez jouer avec cela et partager votre génération la plus étonnante avec le reste des participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un peu de tout\n",
    "output = model.generate(**inputs, num_beams=1, do_sample=True, top_k=40, top_p=0.9, eta_cutoff=0.003, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testez tout\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    num_beams=1, \n",
    "    do_sample=True, \n",
    "    top_k=40, \n",
    "    top_p=0.9, \n",
    "    temperature=0.9,\n",
    "    eta_cutoff=0.003, \n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=50, \n",
    "    pad_token_id=tokenizer.eos_token_id) \n",
    "tokenizer.decode(output[0]) # On convertit les tokens en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesures de diversité de la génération\n",
    "Ici, on va mesurer à quel point générer plusieurs fois va donner des textes différents, n’hésitez pas à jouer avec tous les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sacrebleu\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_bleu(generated_texts):\n",
    "    # Calcul du score BLEU pour chaque paire de textes\n",
    "    pairwise_bleu_scores = []\n",
    "    for i in range(len(generated_texts)):\n",
    "        for j in range(i+1, len(generated_texts)):\n",
    "            hypothesis = generated_texts[i]\n",
    "            references = [generated_texts[j]]\n",
    "            bleu = sacrebleu.corpus_bleu([hypothesis], [references])\n",
    "            pairwise_bleu_scores.append(bleu.score)\n",
    "            #print(f\"score BLEU entre la génération {i} et {j}: {bleu.score:.2f}\")\n",
    "\n",
    "    # Calcul du BLEU moyen en tant que mesure de diversité\n",
    "    average_bleu = sum(pairwise_bleu_scores) / len(pairwise_bleu_scores)\n",
    "    return average_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Aussois is\"  # Prompt\n",
    "\n",
    "# Encodage du prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "# On génère plusieurs textes\n",
    "num_samples = 5  # Nombre de textes\n",
    "max_new_tokens = 50  # Nombre de tokens à générer\n",
    "generated_texts = []\n",
    "\n",
    "# Boucle de génération\n",
    "for i in range(num_samples):\n",
    "    \"\"\" Optionnel, on peut ajouter une seed pour reproduire toujours les mêmes \n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "    \"\"\"\n",
    "\n",
    "    # Génération de textes avec différents paramètres\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=5,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        #top_k=40,       \n",
    "        temperature=0.7,\n",
    "        #top_p=0,\n",
    "        #eta_cutoff=0,\n",
    "        #typical_p=0.9,\n",
    "        #repetition_penalty=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    # Décodage des tokens\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_texts.append(generated_text)\n",
    "    print(f\"Sample {i}:\\n{generated_text}\\n\")\n",
    "\n",
    "avg = calculate_average_bleu(generated_texts)\n",
    "print(f\"Score BLEU moyen :{avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU va mesurer à quel point les phrases sont similaires, plus il est bas, plus les générations sont diverses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam-search curse\n",
    "À partir d’une certaine taille, augmenter le nombre de beams va venir raccourcir la génération, voire renvoyer une phrase vide. Ici on fait de la génération sans trop de contraintes mais avec des tâches bien définies c’est plus fragrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génère plusieurs textes\n",
    "num_samples = 5  # Nombre de textes\n",
    "max_new_tokens = 50  # Nombre de tokens à générer\n",
    "generated_texts = []\n",
    "\n",
    "nb_beams = [10, 20, 50, 100]\n",
    "\n",
    "for j in nb_beams:\n",
    "    lengths = []\n",
    "    # Boucle de génération\n",
    "    for i in range(num_samples):\n",
    "        \"\"\" Optionnel, on peut ajouter une seed pour reproduire toujours les mêmes \n",
    "        torch.manual_seed(i)\n",
    "        np.random.seed(i)\n",
    "        random.seed(i)\n",
    "        \"\"\"\n",
    "\n",
    "        # Génération de textes avec différents paramètres\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=j,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            #top_k=40,       \n",
    "            temperature=1,\n",
    "            #top_p=0,\n",
    "            #eta_cutoff=0,\n",
    "            #typical_p=0.9,\n",
    "            #repetition_penalty=1,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        # Décodage des tokens\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "\n",
    "        # On calcule la longueur du texte généré (en mots)\n",
    "        length = len(generated_text.split())\n",
    "        lengths.append(length)\n",
    "\n",
    "    avg = calculate_average_bleu(generated_texts)\n",
    "    # Calcul de la longueur moyenne pour ce nombre de beams\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    print(f\"Score Bleu moyen pour {j} beams : {avg:.5f}, Longueur moyenne : {avg_length:.2f} mots\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La longueur moyenne des textes a tendance a diminuer quand le nombre de beams devient très grand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions d’arrêt du beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génère plusieurs textes\n",
    "num_samples = 5  # Nombre de textes\n",
    "max_new_tokens = 50  # Nombre de tokens à générer\n",
    "generated_texts = []\n",
    "\n",
    "# Différentes valeurs de early_stopping\n",
    "early_stopping_options = [False, True]\n",
    "\n",
    "# On fixe un nombre de beams\n",
    "num_beams = 20\n",
    "\n",
    "for es in early_stopping_options:\n",
    "    lengths = []\n",
    "    # Boucle de génération\n",
    "    for i in range(num_samples):\n",
    "        \"\"\" Optionnel, on peut ajouter une seed pour reproduire toujours les mêmes \n",
    "        torch.manual_seed(i)\n",
    "        np.random.seed(i)\n",
    "        random.seed(i)\n",
    "        \"\"\"\n",
    "\n",
    "        # Génération de textes avec différents paramètres (vous pouvez les changer à votre guise)\n",
    "        # Ici, on ne varie que early_stopping \n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            early_stopping=es,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        # Décodage des tokens\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "\n",
    "        # On calcule la longueur du texte généré (en mots, approx. via split)\n",
    "        length = len(generated_text.split())\n",
    "        lengths.append(length)\n",
    "\n",
    "    avg = calculate_average_bleu(generated_texts)\n",
    "    # Calcul de la longueur moyenne pour cette configuration\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    print(f\"Pour early_stopping={es}, Score BLEU moyen : {avg:.5f}, Longueur moyenne : {avg_length:.2f} mots\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici on va faire varier la length_penalty pour regarder son effet\n",
    "\n",
    "# On génère plusieurs textes\n",
    "num_samples = 5  # Nombre de textes\n",
    "max_new_tokens = 50  # Nombre de tokens à générer\n",
    "generated_texts = []\n",
    "\n",
    "# On fixe un nombre de beams et on fait varier la length_penalty\n",
    "num_beams = 20\n",
    "length_penalties = [0.8, 1.0, 1.2, 1.5, 2, 3]\n",
    "\n",
    "for lp in length_penalties:\n",
    "    lengths = []\n",
    "    # Boucle de génération\n",
    "    for i in range(num_samples):\n",
    "        \"\"\" Optionnel, on peut ajouter une seed pour reproduire toujours les mêmes \n",
    "        torch.manual_seed(i)\n",
    "        np.random.seed(i)\n",
    "        random.seed(i)\n",
    "        \"\"\"\n",
    "\n",
    "        # Génération de textes avec les paramètres fixes et la length_penalty variable\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=1,\n",
    "            early_stopping=True,       # On garde l'arrêt anticipé\n",
    "            length_penalty=lp,         # On fait varier cette pénalité\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        # Décodage des tokens\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "\n",
    "        # On calcule la longueur du texte généré (en mots, approx. via split)\n",
    "        length = len(generated_text.split())\n",
    "        lengths.append(length)\n",
    "\n",
    "    avg = calculate_average_bleu(generated_texts)\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    print(f\"Score BLEU moyen pour length_penalty={lp} : {avg:.5f}, Longueur moyenne (en mots) : {avg_length:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculs de perplexité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La perplexité est une mesure de surprise du texte, plus elle est élevée moins le modèle associe de fortes probabilités aux mots de la séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Inspiré par l’implémentation HuggingFace disponible ici : https://huggingface.co/docs/transformers/perplexity\n",
    "\n",
    "# Méthode 1, avec tout le contexte à chaque fois \n",
    "def compute_token_by_token_ppl(model, encodings):\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len > model.config.n_positions:\n",
    "        return \"Ce texte est trop long pour la fenêtre de contexte du modèle\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    \n",
    "    logits = outputs.logits  # shape: [batch_size, seq_length, vocab_size]\n",
    "    \n",
    "    # Shift pour prédire le token i à la position i-1\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    # On gather pour regarder les probabilités du token qui est renvoyé\n",
    "    token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    average_nll = -token_log_probs.mean() \n",
    "    token_by_token_ppl = torch.exp(average_nll)\n",
    "    return token_by_token_ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut aller plus loin encore et afficher la probabliité associée à chaque token d’une phrase en modifiant cette fonction :\n",
    "\n",
    "def compute_token_probs_and_ppl(model, tokenizer, encodings, device=device):\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    if seq_len > model.config.n_positions:\n",
    "        return \"Ce texte est trop long pour la fenêtre de contexte du modèle\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "    logits = outputs.logits  # [batch_size, seq_length, vocab_size]\n",
    "\n",
    "    # On shift pour prédire le token t+1 à la position t\n",
    "    shift_logits = logits[:, :-1, :].contiguous()    # Prédictions \n",
    "    shift_labels = input_ids[:, 1:].contiguous()     # tokens prédits\n",
    "\n",
    "    # Log probs de chaque token\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    # Log probs de chaque token prédit\n",
    "    token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)  # [batch_size, seq_length-1]\n",
    "\n",
    "    average_nll = -token_log_probs.mean()\n",
    "    token_by_token_ppl = torch.exp(average_nll)\n",
    "\n",
    "    # On passe des log probs aux probabilités\n",
    "    token_probs = torch.exp(token_log_probs)  # shape: [batch_size, seq_length-1]\n",
    "    token_probs = token_probs.squeeze(0)      # batch_size=1\n",
    "\n",
    "    # On commence à partir du second token de la phrase, le premier n’ayant pas de contexte autre que <s>\n",
    "    predicted_tokens = shift_labels.squeeze(0)  # shape: [seq_length-1]\n",
    "\n",
    "    print(\"Token-by-token probabilities:\")\n",
    "    for i, token_id in enumerate(predicted_tokens):\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        prob = token_probs[i].item()\n",
    "        print(f\"Token: {token_str} | Probability: {prob:.6f}\")\n",
    "\n",
    "    return token_by_token_ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_test = \"The Eiffel Tower is located in\"\n",
    "inputs = tokenizer(prompt_test, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "output = model.generate(**inputs, num_beams=5, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id) # On génère des tokens \n",
    "sortie = tokenizer.decode(output[0]) # On convertit les tokens en mots\n",
    "print(sortie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant à quel point GPT2 est sûr de lui à chaque étape, n’hésitez pas à changer le prompt de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sortie, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "compute_token_probs_and_ppl(model, tokenizer, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, regardons comment faire dans le cas où le contexte est trop grand pour le modèle, le contexte de GPT2 est de 1024 par exemple (j’ai demandé un texte super long à ChatGPT, vous pouvez également proposer le vôtre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"On a crisp autumn morning, Eleanor stepped outside her small cottage at the edge of the forest and began walking down a narrow path lined with amber and russet leaves. The world had changed colors overnight, as if an unseen painter had swept across the landscape with a palette of warm hues. She breathed in the scent of damp earth, distant pine, and the faint aroma of woodsmoke drifting from a neighbor’s chimney. Although the air carried a slight chill, the day promised gentle sunshine and light winds, perfect for a long and thoughtful stroll. As she continued along the winding trail, Eleanor recalled the stories her grandfather had told her when she was a child—tales of hidden groves and forgotten clearings deep in the woods, where ancient trees whispered secrets to one another. He had insisted that if one listened carefully enough, the wind through the branches carried voices from centuries past. She had never been entirely sure whether to believe him, but as she walked, she allowed herself the luxury of imagining these old legends might hold some truth. There was comfort in thinking that one’s ancestors might leave behind faint echoes, lingering in the quiet corners of nature. About half a mile in, Eleanor reached a small stream. Its waters, clear and cold, danced over smooth stones, creating a soft, melodic murmur. She paused to watch leaves float downstream, each one a tiny vessel drifting toward unknown destinations. The sunlight, filtering through the half-bare branches, reflected in bright spots off the ripples, reminding her that beauty often lay in small, transient details. She took her time before moving on, feeling as if each moment was a gift she shouldn’t rush. Not long after crossing the stream via a makeshift wooden plank, she came across a clearing she did not recognize. It was shaped like an oval and ringed with young birch trees whose bark gleamed pale against the darker backdrop of firs and oaks. In the center stood a solitary stone bench, weathered and covered in moss, as though it had been placed there by someone who valued solitude. Eleanor brushed off some of the moss and sat, resting her legs and taking in the silent theater around her. No birds chirped, no squirrels chattered. It was as if this spot had declared itself a sanctuary from noise. While resting, she thought about the countless times she had ventured outdoors in search of nothing in particular: just the quiet companionship of trees, the patient passage of clouds, and the sound of her own footsteps on the trail. She considered how, in these quiet moments, she often found a clarity that eluded her in the hustle of daily life. Back at her cottage, there were chores to be done, letters to answer, and errands waiting. Out here, these demands receded, leaving room for the warmth of memories and the subtle interplay of time and stillness. Refreshed by the pause, Eleanor stood and continued forward, leaving the clearing behind. Eventually, the path widened, and she found herself walking alongside an old stone wall, partially collapsed in places. Vines and moss had claimed it, weaving new textures and patterns that hinted at the slow, persistent artistry of nature over generations. She liked to imagine who might have built the wall—farmers long ago clearing land, or perhaps villagers marking a boundary. The wall, now broken and quiet, bore silent witness to a past she could only guess at. As noon approached, the sunlight grew warmer, and the forest seemed to awaken. A distant woodpecker tapped at a trunk, small birds fluttered between branches, and a gentle breeze carried the distant laughter of someone working in a nearby orchard. Eleanor knew there was a village not far beyond the forest’s border, where life continued its pleasant rhythm: apples harvested, bread baked, stories swapped over steaming cups of tea. Soon, she would turn back toward her cottage, but not just yet. The day still had hours left to unfold. After another half-hour of walking, she reached a hillside that offered a view of rolling fields beyond the trees. Patches of farmland, dotted with hay bales, stretched toward a line of distant hills. A single hawk circled overhead, its keen eyes scanning the ground below for a quick meal. Eleanor watched the hawk’s flight, feeling a quiet admiration for its independence and grace. When she finally turned around to retrace her steps home, she found the forest just as welcoming as before. The path felt familiar but not stale; rather, it was like greeting an old friend. She noticed details she had missed earlier—a cluster of mushrooms at the base of a beech tree, the gentle slope of the trail as it curved around a thicket. Each step brought her closer to the cottage, and with it the ordinary tasks of her life, but she carried within her a renewed sense of peace. By the time Eleanor stepped through her front door, the golden afternoon light had begun to angle across the floorboards. She placed a kettle on the stove, choosing a fragrant herbal blend for her tea. Waiting for the water to boil, she looked out the window at the edge of the forest, grateful that she had taken the time to wander among the trees. There was a quiet magic in that forest—subtle, unassuming, yet undeniably present. It asked for nothing and offered everything: a calm place to think, to listen, to remember. And tomorrow, or perhaps the day after, she might return to discover something new, or to simply be, wrapped in the gentle hush of autumn leaves and whispered histories.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_context_window_ppl(model, encodings, window_size):\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    max_length = window_size\n",
    "\n",
    "    window_losses = []\n",
    "    for start_idx in range(0, seq_len, max_length):\n",
    "        end_idx = min(start_idx + max_length, seq_len)\n",
    "        window_input_ids = input_ids[:, start_idx:end_idx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_input_ids, labels=window_input_ids)\n",
    "        \n",
    "        # outputs.loss représente la NLL moyenne sur tous les tokens dans la fenêtre\n",
    "        window_loss = outputs.loss\n",
    "        window_losses.append(window_loss)\n",
    "\n",
    "    average_loss = torch.stack(window_losses).mean()\n",
    "    context_window_ppl = torch.exp(average_loss)\n",
    "    return context_window_ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_with_half_window_context(model, encodings, window_size):\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    nlls = []\n",
    "    start_positions = range(0, seq_len, half_window)\n",
    "    for start_pos in tqdm(start_positions):\n",
    "        end_pos = start_pos + window_size\n",
    "        # Si on a plus assez de tokens, on break ou on peut calculer la ppl des tokens restants en mettant end_pos = seq_len\n",
    "        if end_pos > seq_len:\n",
    "            end_pos = seq_len\n",
    "            break\n",
    "\n",
    "        # On choisit notre fenêtre\n",
    "        window_input_ids = input_ids[:, start_pos:end_pos]\n",
    "\n",
    "        # Seule la deuxième moitié de la fenêtre de contexte sert à scorer notre texte\n",
    "        target_ids = window_input_ids.clone()\n",
    "        # On masque la première moitié du texte (-100 est une valeur arbitraire)\n",
    "        target_ids[:, :half_window] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        if end_pos == seq_len:\n",
    "            break\n",
    "\n",
    "    # On récupère la perplexité moyenne\n",
    "    average_nll = torch.stack(nlls).mean()\n",
    "    ppl = torch.exp(average_nll)\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_token_by_token_ppl(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity_with_half_window_context(model, inputs, window_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_context_window_ppl(model, inputs, window_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient différents résultats de perplexité à cause des différentes façons de calculer. Sur les texte très longs, ces différences peuvent devenir très grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Bayes Risk Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import sacrebleu\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# On charge le modèle NLLB\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "source_lang = \"eng_Latn\"  # Langue source (Anglais)\n",
    "target_lang = \"fra_Latn\"  # Langue cible (Français)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=source_lang, tgt_lang=target_lang)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Pour NLLB, il faut spécifier la langue cible en préfixant le token <2lang_code>\n",
    "# Ici, on force l'utilisation du bos_token correspondant à la langue cible\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"fra_Latn\")\n",
    "\n",
    "# Texte source à traduire\n",
    "prompt = \"Aussois is a wonderful place\"\n",
    "\n",
    "# Encodage du prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "\n",
    "# Génération des références (candidats \"best\") de manière déterministe\n",
    "nb_best = 5\n",
    "best_set = []\n",
    "for i in range(nb_best):\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        num_beams=5,  # On utilise le beam search pour une génération plus déterministe\n",
    "        forced_bos_token_id=forced_bos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    best_set.append(generated_text)\n",
    "\n",
    "# Génération de candidats divers (monte_carlo_set) de manière stochastique\n",
    "nb_divers = 30\n",
    "monte_carlo_set = []\n",
    "for i in range(nb_divers):\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=10,\n",
    "        top_k=40,  # Pour plus de diversité\n",
    "        forced_bos_token_id=forced_bos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    monte_carlo_set.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_candidate(best_set, monte_carlo_set): \n",
    "    # Ici on cherche le candidat qui ressemble le plus à tous les autres selon le BLEU moyen\n",
    "    scores = []\n",
    "    for candidate in best_set:\n",
    "        pairwise_bleu_scores = []\n",
    "        for reference_text in monte_carlo_set:\n",
    "            bleu = sacrebleu.corpus_bleu([candidate], [[reference_text]])\n",
    "            pairwise_bleu_scores.append(bleu.score)\n",
    "\n",
    "        # Calcul du BLEU moyen pour ce candidat\n",
    "        average_bleu = sum(pairwise_bleu_scores) / len(pairwise_bleu_scores)\n",
    "        scores.append(average_bleu)\n",
    "\n",
    "    # On sélectionne le candidat avec le score moyen le plus élevé\n",
    "    best_index = max(range(len(scores)), key=lambda idx: scores[idx])\n",
    "    return best_set[best_index], scores[best_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidat, score = select_best_candidate(best_set, monte_carlo_set)\n",
    "print(f\"La meilleure traduction est :\\n{candidat}\\navec un BLEU moyen de {score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# On charge le modèle chargé de scorer nos textes\n",
    "# C’est un modèle très simple chargé de donner un score de polarité positif et négatif aux textes, chacun entre 0 et 1\n",
    "scoring_model_name = \"siebert/sentiment-roberta-large-english\"\n",
    "scoring_tokenizer = AutoTokenizer.from_pretrained(scoring_model_name)\n",
    "scoring_model = AutoModelForSequenceClassification.from_pretrained(scoring_model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scoring_model.to(device)\n",
    "scoring_model.eval()\n",
    "\n",
    "def get_positive_score(texts):\n",
    "    \"\"\"\n",
    "    On calcule le sentiment (positif)\n",
    "    \"\"\"\n",
    "    inputs = scoring_tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = scoring_model(**inputs)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    positive_scores = probabilities[:, 1]  # Index 1 représente le score \"positif\"\n",
    "\n",
    "    # Il faut choisir si l’on veut un texte positif ou négatif en commentant la ligne inutile\n",
    "    scores = positive_scores               # On veut que le texte généré soit plutôt positif\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def get_negative_score(texts):\n",
    "    \"\"\"\n",
    "    On calcule le sentiment (négatif)\n",
    "    \"\"\"\n",
    "    inputs = scoring_tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = scoring_model(**inputs)\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    negative_scores = probabilities[:, 0]  # Index 0 représente le score \"négatif\"\n",
    "\n",
    "    # Il faut choisir si l’on veut un texte positif ou négatif en commentant la ligne inutile\n",
    "    scores = negative_scores               # On veut que le texte généré soit plutôt négatif\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Exemple\n",
    "text = \"This movie was terrible\"\n",
    "positive_score = get_positive_score([text])\n",
    "negative_score = get_negative_score([text])\n",
    "print(f\"Positive score: {positive_score[0].item():.4f}\") \n",
    "print(f\"Negative score: {negative_score[0].item():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts_script import main\n",
    "\n",
    "# Arguments\n",
    "c = 1.0              # Constante d’exploration, plus haut = plus d’exploration des noeuds les moins visités\n",
    "alpha = 1.0          # Priorité donnée aux probas du modèle ou au score que l’on renvoie\n",
    "temperature = 0.7    # Température utilisée pendant la génération\n",
    "penalty = 1.1        # Pénalité de répétition\n",
    "num_it = 50          # Nombre d’itérations MCTS par token\n",
    "prompt_text = \"This movie was\"\n",
    "\n",
    "\n",
    "# On va générer un texte \"positif\"\n",
    "main(c, alpha, temperature, penalty, num_it, get_positive_score, prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Même chose avec du texte que l’on veut \"négatif\"\n",
    "\n",
    "main(c, alpha, temperature, penalty, num_it, get_negative_score, prompt_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text_Generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
